import requests
import pandas as pd
from io import BytesIO
from bs4 import BeautifulSoup

# 1. Pobieranie najnowszego XLSX z listÄ… sankcyjnÄ… MF
def get_mf_sanctions():
    url = "https://www.gov.pl/web/finanse/lista-osob-i-podmiotow-wobec-ktorych-stosuje-sie-szczegolne-srodki-ograniczajace-na-podstawie-art-118-ustawy-z-dnia-1-marca-2018-r-o-przeciwdzialaniu-praniu-pieniedzy-i-finansowaniu-terroryzmu"
    
    try:
        print("ğŸ” Sprawdzam stronÄ™ MF:", url)
        print("â³ WysyÅ‚am Å¼Ä…danie HTTP (moÅ¼e potrwaÄ‡ do 30 sekund)...")
        
        # Add user agent to avoid potential blocking
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        page = requests.get(url, timeout=30, headers=headers)
        page.raise_for_status()
        print("âœ… Strona zaÅ‚adowana pomyÅ›lnie")
        
        soup = BeautifulSoup(page.text, "html.parser")
        
        # Debug: sprawdÅº wszystkie linki na stronie
        all_links = soup.find_all("a", href=True)
        print(f"ğŸ“Š Znaleziono {len(all_links)} linkÃ³w na stronie")
        
        # Szukamy linkÃ³w do plikÃ³w Excel/XLSX
        excel_links = []
        for a in all_links:
            href = a.get('href', '')
            if any(ext in href.lower() for ext in ['.xlsx', '.xls', '.excel']):
                excel_links.append(href)
                print(f"ğŸ“ Znaleziono link Excel: {href}")
        
        # JeÅ›li nie ma bezpoÅ›rednich linkÃ³w .xlsx, szukaj innych formatÃ³w
        if not excel_links:
            print("âš ï¸  Nie znaleziono bezpoÅ›rednich linkÃ³w Excel, szukam alternatyw...")
            
            # Szukaj linkÃ³w zawierajÄ…cych sÅ‚owa kluczowe
            keywords = ['sankcje', 'lista', 'xlsx', 'excel', 'download', 'pobierz']
            for a in all_links:
                href = a.get('href', '')
                text = a.get_text().lower()
                if any(keyword in text or keyword in href.lower() for keyword in keywords):
                    print(f"ğŸ”— Potencjalny link: {href} (tekst: {text[:50]}...)")
        
        # Wybierz pierwszy znaleziony link Excel
        link = excel_links[0] if excel_links else None
        
        if not link:
            # SprÃ³buj alternatywnych ÅºrÃ³deÅ‚
            print("ğŸ”„ PrÃ³bujÄ™ alternatywnych ÅºrÃ³deÅ‚...")
            alternative_urls = [
                "https://www.gov.pl/web/finanse/sankcje",
                "https://www.gov.pl/web/finanse/lista-sankcji",
                "https://www.gov.pl/web/finanse/lista-osob-i-podmiotow"
            ]
            
            for alt_url in alternative_urls:
                try:
                    print(f"ğŸ” Sprawdzam alternatywnÄ… stronÄ™: {alt_url}")
                    alt_page = requests.get(alt_url, timeout=30, headers=headers)
                    alt_soup = BeautifulSoup(alt_page.text, "html.parser")
                    
                    for a in alt_soup.find_all("a", href=True):
                        href = a.get('href', '')
                        if any(ext in href.lower() for ext in ['.xlsx', '.xls']):
                            link = href
                            print(f"âœ… Znaleziono link na alternatywnej stronie: {link}")
                            break
                    
                    if link:
                        break
                except Exception as e:
                    print(f"âŒ BÅ‚Ä…d przy sprawdzaniu {alt_url}: {e}")
                    continue
        
        if not link:
            print("âš ï¸  Nie znaleziono linku do pliku XLSX na Å¼adnej ze stron MF")
            print("ğŸ” Sprawdzam zaÅ‚Ä…czniki na stronie...")
            
            # Check for attachment links that might contain the data
            attachment_links = []
            for a in all_links:
                href = a.get('href', '')
                text = a.get_text().strip().lower()
                if '/attachment/' in href and ('lista' in text or 'sankcje' in text):
                    attachment_links.append(href)
                    print(f"ğŸ“ Znaleziono zaÅ‚Ä…cznik: {href}")
            
            if attachment_links:
                print(f"ğŸ”„ PrÃ³bujÄ™ pobraÄ‡ dane z zaÅ‚Ä…cznikÃ³w...")
                for attachment_url in attachment_links:
                    try:
                        if attachment_url.startswith("/"):
                            attachment_url = "https://www.gov.pl" + attachment_url
                        
                        print(f"ğŸ“¥ Pobieram zaÅ‚Ä…cznik: {attachment_url}")
                        response = requests.get(attachment_url, timeout=30, headers=headers)
                        response.raise_for_status()
                        
                        # Check content type first
                        content_type = response.headers.get('Content-Type', '').lower()
                        print(f"ğŸ“Š Content-Type: {content_type}")
                        
                        # Try to read as Excel
                        if 'excel' in content_type or 'spreadsheet' in content_type or response.content.startswith(b'PK'):
                            try:
                                import pandas as pd
                                df_mf = pd.read_excel(BytesIO(response.content))
                                print(f"âœ… PomyÅ›lnie pobrano {len(df_mf)} rekordÃ³w z zaÅ‚Ä…cznika Excel")
                                print(f"ğŸ“‹ Kolumny: {list(df_mf.columns)}")
                                return df_mf
                            except Exception as excel_error:
                                print(f"âŒ BÅ‚Ä…d odczytu Excel: {excel_error}")
                                continue
                        else:
                            print(f"âš ï¸  ZaÅ‚Ä…cznik nie jest plikiem Excel (Content-Type: {content_type})")
                            continue
                    except Exception as e:
                        print(f"âŒ BÅ‚Ä…d przy pobieraniu zaÅ‚Ä…cznika: {e}")
                        continue
            
            raise Exception("Nie znaleziono linku do pliku XLSX na Å¼adnej ze stron MF")
        
        # Czasem link jest wzglÄ™dny, dodaj domenÄ™
        if link.startswith("/"):
            link = "https://www.gov.pl" + link
        elif not link.startswith("http"):
            link = "https://www.gov.pl" + "/" + link
        
        print("ğŸ“¥ Pobieram plik MF:", link)
        response = requests.get(link, timeout=60, headers=headers)
        response.raise_for_status()
        
        df_mf = pd.read_excel(BytesIO(response.content))
        print(f"âœ… PomyÅ›lnie pobrano {len(df_mf)} rekordÃ³w z MF")
        return df_mf
        
    except requests.exceptions.RequestException as e:
        raise Exception(f"BÅ‚Ä…d poÅ‚Ä…czenia z serwerem MF: {e}")
    except pd.errors.ExcelFileError as e:
        raise Exception(f"BÅ‚Ä…d odczytu pliku Excel: {e}")
    except Exception as e:
        raise Exception(f"Nieoczekiwany bÅ‚Ä…d: {e}")


# 2. Pobieranie listy sankcyjnej MSWiA
def get_mswia_sanctions():
    mswia_url = "https://www.gov.pl/web/mswia/lista-osob-i-podmiotow-objetych-sankcjami"
    
    try:
        print("ğŸ” Sprawdzam stronÄ™ MSWiA:", mswia_url)
        print("â³ WysyÅ‚am Å¼Ä…danie HTTP (moÅ¼e potrwaÄ‡ do 30 sekund)...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        page = requests.get(mswia_url, timeout=30, headers=headers)
        page.raise_for_status()
        print("âœ… Strona MSWiA zaÅ‚adowana pomyÅ›lnie")
        
        soup = BeautifulSoup(page.text, "html.parser")
        
        # ZnajdÅº wszystkie linki na stronie
        all_links = soup.find_all("a", href=True)
        print(f"ğŸ“Š Znaleziono {len(all_links)} linkÃ³w na stronie MSWiA")
        
        # Szukamy linkÃ³w do plikÃ³w Excel/XLSX - szczegÃ³lnie w sekcji "MateriaÅ‚y"
        excel_links = []
        
        # Najpierw szukaj bezpoÅ›rednich linkÃ³w .xlsx
        for a in all_links:
            href = a.get('href', '')
            text = a.get_text().strip().lower()
            if any(ext in href.lower() for ext in ['.xlsx', '.xls', '.excel']):
                excel_links.append((href, text))
                print(f"ğŸ“ Znaleziono link Excel MSWiA: {href}")
        
        # JeÅ›li nie ma bezpoÅ›rednich linkÃ³w, szukaj w sekcji "MateriaÅ‚y" lub linkÃ³w z tekstem o sankcjach
        if not excel_links:
            print("âš ï¸  Nie znaleziono bezpoÅ›rednich linkÃ³w Excel, szukam w sekcji MateriaÅ‚y...")
            
            # Szukaj linkÃ³w zawierajÄ…cych sÅ‚owa kluczowe zwiÄ…zane z sankcjami
            keywords = ['sankcje', 'lista', 'tabela', 'xlsx', 'excel', 'download', 'pobierz', 'attachment', 'materiaÅ‚y']
            for a in all_links:
                href = a.get('href', '')
                text = a.get_text().strip().lower()
                
                # SprawdÅº czy link zawiera sÅ‚owa kluczowe lub jest w sekcji materiaÅ‚Ã³w
                if any(keyword in text or keyword in href.lower() for keyword in keywords):
                    print(f"ğŸ”— Potencjalny link MSWiA: {href} (tekst: '{text[:50]}...')")
                    
                    # Dodaj link jeÅ›li to plik Excel lub zaÅ‚Ä…cznik
                    if any(ext in href.lower() for ext in ['.xlsx', '.xls']) or '/attachment/' in href:
                        excel_links.append((href, text))
                        print(f"âœ… Dodano link Excel: {href}")
        
        # JeÅ›li nadal nie ma linkÃ³w, sprÃ³buj znaleÅºÄ‡ linki z konkretnym wzorcem nazwy pliku
        if not excel_links:
            print("âš ï¸  Szukam linkÃ³w z wzorcem nazwy pliku sankcyjnego...")
            for a in all_links:
                href = a.get('href', '')
                text = a.get_text().strip().lower()
                
                # Szukaj linkÃ³w z wzorcem "tabela_lista_sankcyjna" lub podobnym
                if ('tabela' in text and 'sankcyjna' in text) or 'lista_sankcyjna' in href.lower():
                    print(f"ğŸ”— Znaleziono link z wzorcem sankcyjnym: {href} (tekst: '{text[:50]}...')")
                    excel_links.append((href, text))
                    break
        
        # Wybierz link z listÄ… sankcyjnÄ… (preferuj linki z tekstem "lista sankcyjna" lub "tabela_lista")
        link = None
        if excel_links:
            # Szukaj linku z tekstem "lista sankcyjna" lub "tabela_lista"
            for potential_link, potential_text in excel_links:
                # SprawdÅº czy tekst zawiera sÅ‚owa kluczowe zwiÄ…zane z listÄ… sankcyjnÄ…
                if any(keyword in potential_text for keyword in ['lista sankcyjna', 'tabela_lista', 'lista', 'sankcyjna', 'tabela']):
                    link = potential_link
                    print(f"âœ… Wybrano link z listÄ… sankcyjnÄ…: {link} (tekst: '{potential_text[:50]}...')")
                    break
            
            # JeÅ›li nie znaleziono linku z listÄ… sankcyjnÄ…, wybierz pierwszy dostÄ™pny
            if not link:
                link, text = excel_links[0]
                print(f"âš ï¸  Wybrano pierwszy dostÄ™pny link: {link} (tekst: '{text[:50]}...')")
        
        if not link:
            raise Exception("Nie znaleziono linku do pliku XLSX na stronie MSWiA")
        
        # Czasem link jest wzglÄ™dny, dodaj domenÄ™
        if link.startswith("/"):
            link = "https://www.gov.pl" + link
        elif not link.startswith("http"):
            link = "https://www.gov.pl" + "/" + link
        
        print("ğŸ“¥ Pobieram plik MSWiA:", link)
        response = requests.get(link, timeout=60, headers=headers)
        response.raise_for_status()
        
        # Check content type first
        content_type = response.headers.get('Content-Type', '').lower()
        print(f"ğŸ“Š Content-Type: {content_type}")
        
        # Try to read as Excel
        if 'excel' in content_type or 'spreadsheet' in content_type or response.content.startswith(b'PK'):
            try:
                import pandas as pd
                df_mswia = pd.read_excel(BytesIO(response.content))
                print(f"âœ… PomyÅ›lnie pobrano {len(df_mswia)} rekordÃ³w z MSWiA")
                print(f"ğŸ“‹ Kolumny: {list(df_mswia.columns)}")
                return df_mswia
            except Exception as excel_error:
                print(f"âŒ BÅ‚Ä…d odczytu Excel MSWiA: {excel_error}")
                raise
        else:
            print(f"âš ï¸  Plik MSWiA nie jest plikiem Excel (Content-Type: {content_type})")
            raise Exception(f"Nieoczekiwany typ pliku: {content_type}")
        
    except requests.exceptions.RequestException as e:
        raise Exception(f"BÅ‚Ä…d poÅ‚Ä…czenia z serwerem MSWiA: {e}")
        
    except Exception as e:
        raise Exception(f"Nieoczekiwany bÅ‚Ä…d przy pobieraniu danych MSWiA: {e}")


# 3. Pobieranie listy sankcyjnej UE (CSV z portalu data.europa.eu)
def get_eu_sanctions():
    eu_url = "https://webgate.ec.europa.eu/fsd/fsf/public/files/csvFullSanctionsList/content?token=dummy"
    
    try:
        print("ğŸ“¥ Pobieram listÄ™ UE:", eu_url)
        response = requests.get(eu_url, timeout=60)
        response.raise_for_status()
        
        df_eu = pd.read_csv(BytesIO(response.content), sep=";")
        print(f"âœ… PomyÅ›lnie pobrano {len(df_eu)} rekordÃ³w z UE")
        return df_eu
        
    except requests.exceptions.RequestException as e:
        raise Exception(f"BÅ‚Ä…d poÅ‚Ä…czenia z serwerem UE: {e}")
        
    except pd.errors.ParserError as e:
        raise Exception(f"BÅ‚Ä…d parsowania pliku CSV UE: {e}")
        
    except Exception as e:
        raise Exception(f"Nieoczekiwany bÅ‚Ä…d przy pobieraniu danych UE: {e}")


